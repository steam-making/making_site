import requests
import re
import time
from bs4 import BeautifulSoup
from datetime import datetime, date
from urllib.parse import urljoin

from .models import CollectedPost



AJAX_URL = "https://neulbomhub.kosac.re.kr/prrg/papr/papr/paprListPgng.do"
BASE_URL = "https://neulbomhub.kosac.re.kr"

# ğŸ”¥ ìˆ˜ì§‘ ê¸°ì¤€ ë‚ ì§œ (ì´ ë‚ ì§œ ì´í›„ ê³µê³ ë§Œ)
CUTOFF_DATE = date(2025, 11, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     


def get_total_pages(soup):
    """
    í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
    """
    buttons = soup.select("div.paging button[onclick]")
    pages = []

    for btn in buttons:
        onclick = btn.get("onclick", "")
        match = re.search(r"go_Page\((\d+)\)", onclick)
        if match:
            pages.append(int(match.group(1)))

    return max(pages) if pages else 1


# ==================================================
# âœ… ëŠ˜ë´„í—ˆë¸Œ ìƒì„¸ í˜ì´ì§€ íŒŒì‹±
# ==================================================
def fetch_neulbom_detail(detail_url, headers=None):
    """
    ëŠ˜ë´„í—ˆë¸Œ ìƒì„¸ í˜ì´ì§€ì—ì„œ
    - ìš´ì˜ìš”ì¼ (â— í‘œì‹œ ê¸°ì¤€)
    - ì²¨ë¶€íŒŒì¼ URL
    """
    try:
        res = requests.get(detail_url, headers=headers, timeout=60)
        res.raise_for_status()
    except Exception:
        return "", []

    soup = BeautifulSoup(res.text, "html.parser")

    # ==================================================
    # ğŸ”¹ ìš´ì˜ìš”ì¼ ì¶”ì¶œ (â— í‘œì‹œ ê¸°ì¤€)
    # ==================================================
    weekday_list = []

    # "ìš”ì¼/ì‹œê°„ëŒ€" thë¥¼ ê°€ì§„ tr ì°¾ê¸°
    th = soup.find("th", string=lambda x: x and "ìš”ì¼/ì‹œê°„ëŒ€" in x)
    if th:
        table = th.find_next("table")
        if table:
            # ìš”ì¼ í—¤ë” (ì›”~ê¸ˆ)
            headers = [h.get_text(strip=True) for h in table.select("thead th")[1:]]
            # headers ì˜ˆ: ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ"]

            marked_indexes = set()

            for row in table.select("tbody tr"):
                tds = row.select("td")
                for idx, td in enumerate(tds):
                    if "â—" in td.get_text():
                        marked_indexes.add(idx)

            for idx in sorted(marked_indexes):
                if idx < len(headers):
                    weekday_list.append(headers[idx])

    weekday = ", ".join(weekday_list)

    # ==================================================
    # ğŸ”¹ ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
    # ==================================================
    attachments = []
    for a in soup.select("a[href*='fileDownLoad.do']"):
        href = a.get("href")
        if not href:
            continue
        attachments.append({
            "name": a.get_text(strip=True) or "ì²¨ë¶€íŒŒì¼",
            "url": urljoin("https://neulbomhub.kosac.re.kr", href),
        })

    return weekday, attachments



def collect_posts(site):
    """
    SourceSite.collector ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì§‘ ë°©ì‹ ë¶„ê¸°
    """
    if site.collector == "NEULBOM":
        return collect_neulbom(site)

    if site.collector == "JNE_COMMON":
        return collect_jne_common(site)

    if site.collector == "JNE_REGION":
        return collect_jne_region(site)

    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” collector íƒ€ì…: {site.collector}")


from .models import NeulbomConfig

def get_cutoff_date():
    config = NeulbomConfig.objects.first()
    return config.cutoff_date if config else None

# ==================================================
# âœ… ëŠ˜ë´„í—ˆë¸Œ ìˆ˜ì§‘ (ìƒì„¸ í¬í•¨)
# ==================================================
def collect_neulbom(site):
    headers = {
        "User-Agent": "Mozilla/5.0",
        "X-Requested-With": "XMLHttpRequest",
        "Content-Type": "application/x-www-form-urlencoded",
    }

    detail_headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://neulbomhub.kosac.re.kr/",
        "Accept-Language": "ko-KR,ko;q=0.9",
    }

    cutoff_date = get_cutoff_date()

    new_saved = 0
    # ==================================================
    # ğŸ”¥ NEW ì´ˆê¸°í™” (í•˜ë£¨ ì§€ë‚œ ê¸€)
    # ==================================================
    today = date.today()

    CollectedPost.objects.filter(
        source=site,
        is_new=True,
        collected_at__date__lt=today
    ).update(is_new=False)

    # ğŸ”¹ 1í˜ì´ì§€ â†’ ì „ì²´ í˜ì´ì§€ ê³„ì‚°
    first_payload = {
        "list_oper_rgn_lclsf_cd": "R0500",
        "miv_pageNo": 1,
    }

    first_res = requests.post(AJAX_URL, data=first_payload, headers=headers, timeout=60)
    first_res.raise_for_status()

    soup = BeautifulSoup(first_res.text, "html.parser")
    total_pages = get_total_pages(soup)

    for page in range(1, total_pages + 1):
        payload = {
            "list_oper_rgn_lclsf_cd": "R0500",
            "miv_pageNo": page,
        }

        response = requests.post(AJAX_URL, data=payload, headers=headers, timeout=60)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        rows = soup.select("div.boardlist table tbody tr")

        for row in rows:
            flex_tds = row.select("td.flex")
            if len(flex_tds) < 4:
                continue

            def clean(td):
                label = td.select_one("span.w_hidden")
                if label:
                    label.decompose()
                return td.get_text(strip=True)

            region = clean(flex_tds[0])
            school = clean(flex_tds[1])

            # ğŸ”¥ ê´‘ì£¼ë§Œ
            if not region.startswith("ê´‘ì£¼ê´‘ì—­ì‹œ"):
                continue

            title_el = row.select_one("td.title_box .title a")
            if not title_el:
                continue

            onclick = title_el.get("onclick", "")
            match = re.search(r"paprDetail\('E',\s*'([^']+)'\)", onclick)
            if not match:
                continue

            mng_no = match.group(1)
            title = title_el.get_text(strip=True)
            period_text = clean(flex_tds[3]).replace("ëª¨ì§‘ê¸°ê°„", "").strip()
            status = clean(flex_tds[-1]) if flex_tds else ""

            # ğŸ”¹ ëª¨ì§‘ê¸°ê°„ íŒŒì‹± (ì¢…ë£Œì¼ ê¸°ì¤€)
            try:
                start_str, end_str = [x.strip() for x in period_text.split("~")]
                end_date = datetime.strptime(end_str, "%Y-%m-%d").date()
            except Exception:
                continue

            # âŒ ëª¨ì§‘ ì¢…ë£Œëœ ê³µê³  ìŠ¤í‚µ
            if end_date < today:
                continue

            # âŒ ê¸°ì¤€ì¼ ì´ì „ ê³µê³  ìŠ¤í‚µ (ğŸ”¥ í•µì‹¬)
            if cutoff_date and end_date < cutoff_date:
                continue


            link = (
                "https://neulbomhub.kosac.re.kr/prrg/papr/papr/paprDetail.do"
                f"?mode=E&prgrm_pbanc_mng_no={mng_no}"
            )

            # ğŸ”¥ ìƒì„¸ í˜ì´ì§€ â†’ ìš”ì¼ + ì²¨ë¶€íŒŒì¼ (í•­ìƒ ìµœì‹ )
            weekday, attachments = fetch_neulbom_detail(
                link,
                headers=detail_headers
            )

            # âŒ ì²¨ë¶€íŒŒì¼ ì—†ëŠ” ê³µê³ ëŠ” ìˆ˜ì§‘ ì œì™¸ (ê´‘ì£¼ ëŠ˜ë´„)
            if not attachments:
                continue

            # ğŸ”¥ create or update
            obj, created = CollectedPost.objects.update_or_create(
                source=site,
                mng_no=mng_no,
                defaults={
                    "title": title,
                    "link": link,
                    "region": region,
                    "school_name": school,
                    "post_date": period_text,
                    "status": status,
                    "weekday": weekday,
                    "attachment_urls": attachments,
                }
            )

            # âœ… ì‹ ê·œë§Œ NEW ì²˜ë¦¬
            if created:
                obj.is_new = True
                obj.save(update_fields=["is_new"])


            time.sleep(0.2)

        time.sleep(0.2)

    return new_saved


# ==================================================
# JNE COMMON (ê¸°ì¡´ ìœ ì§€)
# ==================================================
def collect_jne_common(site):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/121.0.0.0 Safari/537.36"
        ),
        "Referer": site.url,
        "Accept-Language": "ko-KR,ko;q=0.9",
    }

    existing_mng = set(
        CollectedPost.objects
        .filter(source=site)
        .values_list("mng_no", flat=True)
    )

    new_saved = 0
    page = 1
    MAX_PAGES = 50

    while page <= MAX_PAGES:
        params = site.extra_params.copy() if site.extra_params else {}
        params["pageIndex"] = page

        try:
            response = requests.get(site.url, params=params, headers=headers, timeout=60)
            response.raise_for_status()
        except Exception:
            break

        soup = BeautifulSoup(response.text, "html.parser")
        rows = soup.select(site.list_selector)
        if not rows:
            break

        for row in rows:
            school_el = row.select_one("td:nth-child(2)")
            school_name = school_el.get_text(strip=True) if school_el else ""

            title_el = row.select_one("td.al a.nttInfoBtn")
            if not title_el:
                continue

            title = title_el.get_text(strip=True)
            mng_no = title_el.get("data-id")
            if not mng_no or mng_no in existing_mng:
                continue

            link = (
                "https://www.jne.go.kr/aftersc/na/ntt/selectNttInfo.do"
                f"?bbsId=367&nttSn={mng_no}&mi=762"
            )

            date_el = row.select_one("td:nth-child(5)")
            post_date = date_el.get_text(strip=True) if date_el else ""

            CollectedPost.objects.create(
                source=site,
                mng_no=mng_no,
                title=title,
                link=link,
                region="ì „ë¼ë‚¨ë„",
                school_name=school_name,
                post_date=post_date,
                status="ë¬´ê´€",
                is_new=True,
            )

            existing_mng.add(mng_no)
            new_saved += 1

        page += 1
        time.sleep(0.5)

    return new_saved


# ==================================================
# JNE REGION (ê¸°ì¡´ ìœ ì§€)
# ==================================================
def collect_jne_region(site):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/121.0.0.0 Safari/537.36"
        ),
        "Referer": site.url,
        "Accept-Language": "ko-KR,ko;q=0.9",
    }

    existing_mng = set(
        CollectedPost.objects
        .filter(source=site)
        .values_list("mng_no", flat=True)
    )

    new_saved = 0
    page = 1
    MAX_PAGES = 10

    while page <= MAX_PAGES:
        params = {
            "bbsId": site.extra_params.get("bbsId"),
            "mi": site.extra_params.get("mi"),
            "pageIndex": page,
        }

        response = requests.get(site.url, params=params, headers=headers, timeout=60)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        rows = soup.select("tbody tr")
        if not rows:
            break

        saved_in_page = 0

        for row in rows:
            title_el = row.select_one("a.nttInfoBtn")
            if not title_el:
                continue

            title = title_el.get_text(strip=True)
            mng_no = title_el.get("data-id")
            if not mng_no or mng_no in existing_mng:
                continue

            tds = row.select("td")
            if len(tds) < 4:
                continue

            school_name = tds[2].get_text(strip=True)
            post_date = tds[3].get_text(strip=True)

            link = urljoin(
                site.url,
                f"/hped/na/ntt/selectNttInfo.do"
                f"?bbsId={site.extra_params.get('bbsId')}"
                f"&nttSn={mng_no}"
                f"&mi={site.extra_params.get('mi')}"
            )

            CollectedPost.objects.create(
                source=site,
                mng_no=mng_no,
                title=title,
                link=link,
                region="ì „ë¼ë‚¨ë„",
                school_name=school_name,
                post_date=post_date,
                status="",
                is_new=True,
            )

            existing_mng.add(mng_no)
            new_saved += 1
            saved_in_page += 1

        if saved_in_page == 0:
            break

        page += 1
        time.sleep(0.1)

    print(f"[DONE][JNE_REGION] ì‹ ê·œ ìˆ˜ì§‘ {new_saved}ê±´")
    return new_saved

